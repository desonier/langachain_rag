# ğŸŒ Resume RAG System - Streamlit Web Interface

## ğŸš€ Quick Start

Launch the web application:
```bash
streamlit run streamlit_app.py
```

The app will open at: http://localhost:8501

## ğŸ“‹ Features Overview

### ğŸ“¥ **Ingest Tab**
Upload and process resume files into the vector database.

**Key Features:**
- âœ… **File Upload**: Drag & drop PDF/DOCX resume files
- âœ… **LLM-Assisted Parsing**: Toggle AI-powered content extraction
- âœ… **Batch Processing**: Upload multiple files at once
- âœ… **Force Update**: Override existing resumes
- âœ… **Real-time Progress**: Watch processing in real-time
- âœ… **Database Statistics**: View resume counts and chunks
- âœ… **Resume Listing**: Browse processed resumes

### ğŸ” **Query Tab**
Search and query resume information with AI-powered responses.

**Key Features:**
- âœ… **Natural Language Queries**: Ask questions in plain English
- âœ… **Multiple Query Types**:
  - Search all resumes
  - Target specific resume by ID
  - Filter by file format (PDF/DOCX)
- âœ… **Source Documents**: See which resume chunks provided answers
- âœ… **Rich Metadata**: View section names, chunk types, and more
- âœ… **Database Overview**: Quick stats and resume listing

## ğŸ¯ Usage Guide

### Step 1: Configure Environment
Ensure your `.env` file contains:
```env
AZURE_OPENAI_ENDPOINT=your_endpoint
AZURE_OPENAI_KEY=your_key
AZURE_OPENAI_API_VERSION=your_version
EMBEDDING_MODEL=text-embedding-ada-002
AZURE_OPENAI_CHATGPT_DEPLOYMENT=your_deployment
```

### Step 2: Ingest Resumes
1. Go to the **ğŸ“¥ Ingest** tab
2. Configure settings:
   - Set database path (default: `./resume_vectordb`)
   - Enable/disable LLM-assisted parsing
   - Choose force update if needed
3. Click **"Initialize Pipeline"**
4. Upload resume files (PDF/DOCX)
5. Click **"ğŸš€ Process Files"**
6. Watch real-time processing progress

### Step 3: Query Resumes
1. Go to the **ğŸ” Query** tab
2. Click **"Initialize Query System"**
3. Enter your question in the text area
4. Choose query type:
   - **All Resumes**: Search across all documents
   - **Specific Resume**: Target one resume by ID
   - **Filter by Format**: Search only PDF or DOCX files
5. Click **"ğŸ” Search"**
6. Review the AI-generated answer and source documents

## ğŸ’¡ Example Queries

### General Questions
- "What are Brandon's key skills and technical expertise?"
- "How many years of experience does this candidate have?"
- "What certifications does Brandon hold?"
- "What is Brandon's educational background?"

### Specific Searches
- "Find candidates with cybersecurity experience"
- "Who has Security+ certification?"
- "Show me penetration testing skills"
- "What programming languages are mentioned?"

### Metadata Searches
- Search only PDF resumes: Use "Filter by Format" â†’ PDF
- Target specific resume: Use Resume ID from ingest tab
- Find recent job titles or contact information

## ğŸ”§ Advanced Features

### ğŸ¤– LLM-Assisted Parsing
When enabled, the system extracts:
- **Candidate Name**: Full name identification
- **Contact Info**: Email, phone, location
- **Key Skills**: Technical and professional skills
- **Experience Years**: Estimated total experience
- **Education**: Degree and institution
- **Certifications**: Professional credentials
- **Job Titles**: Recent positions
- **Industries**: Relevant domains

### ğŸ“Š Rich Metadata
Each resume chunk includes:
```json
{
  "candidate_name": "Brandon J. Tobalski",
  "key_skills": "Cybersecurity, Leadership...",
  "experience_years": 13,
  "section_name": "Core Competencies",
  "chunk_type": "semantic_section",
  "parsing_method": "llm_assisted"
}
```

### ğŸ¯ Semantic Chunking
Instead of arbitrary splits, creates logical sections:
- Contact Information
- Professional Summary
- Core Competencies  
- Work Experience
- Education & Training
- Certifications

## ğŸ› ï¸ Sidebar Controls

### Configuration Panel
- âœ… **Environment Check**: Validates required Azure OpenAI settings
- ğŸ—‘ï¸ **Clear Database**: Remove all processed resumes
- ğŸ“Š **Quick Stats**: Database overview

### Database Management
- **Initialize Systems**: Set up ingest/query pipelines
- **View Statistics**: Resume counts, chunk statistics
- **List Resumes**: Browse processed documents
- **Clear Data**: Fresh start option

## ğŸš¨ Troubleshooting

### Common Issues

**"Missing environment variables"**
- Check `.env` file exists and contains required Azure OpenAI settings
- Restart Streamlit after updating environment

**"Failed to initialize ingest pipeline"**
- Verify Azure OpenAI credentials are correct
- Check network connectivity
- Try disabling LLM parsing for faster setup

**"Database not found"**
- Run ingest process first before querying
- Check database path is correct
- Initialize ingest pipeline before query system

**Upload errors**
- Ensure files are PDF or DOCX format
- Check file sizes aren't too large
- Verify files aren't corrupted

### Performance Tips

**For Faster Processing:**
- Disable LLM-assisted parsing for speed
- Process files in smaller batches
- Use simpler resume formats

**For Better Results:**
- Enable LLM-assisted parsing
- Use well-formatted resumes
- Include complete contact and experience information

## ğŸ‰ Benefits

### User Experience
- ğŸŒ **Web Interface**: No command-line needed
- ğŸ“± **Responsive Design**: Works on desktop and mobile
- ğŸ”„ **Real-time Updates**: Live progress and results
- ğŸ“Š **Visual Statistics**: Charts and metrics

### Functionality
- ğŸ¤– **AI-Powered**: LLM extraction and semantic search
- ğŸ“ **Batch Processing**: Handle multiple files efficiently
- ğŸ” **Advanced Search**: Multiple query types and filters
- ğŸ“š **Source Attribution**: See exactly where answers come from

### Productivity  
- âš¡ **Quick Setup**: Initialize with one click
- ğŸ¯ **Targeted Queries**: Find specific information fast
- ğŸ“ˆ **Database Management**: Easy monitoring and maintenance
- ğŸ”„ **Iterative Workflow**: Ingest more, query immediately

Launch the app and start building your intelligent resume database! ğŸš€